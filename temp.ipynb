{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_pkl = '/media/ddd/git/spa_AeDet/data/nuScenes/nuscenes_12hz_infos_train.pkl'\n",
    "train_np = np.load(train_pkl, allow_pickle=True)\n",
    "split7_train_np = train_np[::7]\n",
    "np.save('/media/ddd/git/spa_AeDet/data/nuScenes/nuscenes_12hz_infos_train_7split.pkl', split7_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ddd/anaconda3/envs/depth/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 101: invalid device ordinal (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import mmcv\n",
    "train_pkl = '/media/ddd/git/spa_AeDet/data/nuScenes/nuscenes_12hz_infos_train.pkl'\n",
    "train_np = mmcv.load(train_pkl,)\n",
    "split7_train_np = train_np[::7]\n",
    "mmcv.dump(split7_train_np, '/media/ddd/git/spa_AeDet/data/nuScenes/nuscenes_12hz_infos_train_7split.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "train_pkl = '/media/ddd/git/spa_AeDet/data/nuScenes/nuscenes_12hz_infos_train.pkl'\n",
    "train_np = mmcv.load(train_pkl,)\n",
    "split7_train_np = train_np[::7]\n",
    "mmcv.dump(split7_train_np, '/media/ddd/git/spa_AeDet/data/nuScenes/nuscenes_12hz_infos_train_7split.pkl')\n",
    "split7_train_np = train_np[15050:15150]\n",
    "mmcv.dump(split7_train_np, '/media/ddd/git/spa_AeDet/data/nuScenes/nuscenes_12hz_infos_train_100data.pkl')\n",
    "split7_train_np = train_np[15050:15070]\n",
    "mmcv.dump(split7_train_np, '/media/ddd/git/spa_AeDet/data/nuScenes/nuscenes_12hz_infos_train_20data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "split7_train_np = train_np[15050:15060]\n",
    "mmcv.dump(split7_train_np, '/media/ddd/git/spa_AeDet/data/nuScenes/nuscenes_12hz_infos_val_10data.pkl')\n",
    "print(len(split7_train_np))\n",
    "for i in range(9):\n",
    "    split7_train_np += train_np[15050:15060]\n",
    "print(len(split7_train_np))\n",
    "mmcv.dump(split7_train_np, '/media/ddd/git/spa_AeDet/data/nuScenes/nuscenes_12hz_infos_train_100data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 7, 7])\n",
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255])\n",
      "torch.Size([2, 64, 256, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "#kernel_size\n",
    "kernel_num = 7\n",
    "#y의 위치\n",
    "k = 3\n",
    "#x의 위치\n",
    "x = 10\n",
    "stride = 1\n",
    "h, w = 64,256 \n",
    "\n",
    "indices = torch.arange(1, 65) \n",
    "off_mat = 32 / indices  \n",
    "\n",
    "out_h, out_w = h // stride, w // stride \n",
    "kernel_div2 = int(kernel_num//2)\n",
    "\n",
    "values = torch.arange(out_w)\n",
    "M = values.view(out_w, 1, 1).repeat(1, kernel_num, kernel_num)\n",
    "# print(values.view(out_w, 1, 1))\n",
    "M = M.unsqueeze(0).repeat(64, 1, 1, 1)                         \n",
    "\n",
    "\n",
    "kernel_offset = torch.arange(-int(kernel_num//2), kernel_num//2 + 1)\n",
    "kernel_offset = kernel_offset.unsqueeze(0).repeat(kernel_num,1).unsqueeze(0).unsqueeze(0)\n",
    "M = M + kernel_offset\n",
    "print(M.shape)\n",
    "print(M[0][:,5,3])\n",
    "kernel = torch.arange(-int(kernel_num//2), kernel_num//2 + 1)\n",
    "cnn_matrix = torch.zeros(out_w, kernel_num)\n",
    "cnn_matrix = (off_mat.view(-1, 1) * kernel).float()\n",
    "\n",
    "#print(cnn_matrix.shape)\n",
    "\n",
    "first_row = cnn_matrix[0].unsqueeze(0).repeat(int(kernel_num//2), 1)\n",
    "input_tensor = torch.cat((first_row, cnn_matrix), dim=0)\n",
    "last_row = cnn_matrix[-1].unsqueeze(0).repeat(int(kernel_num//2), 1)\n",
    "input_tensor = torch.cat((input_tensor, last_row), dim=0)\n",
    "\n",
    "cnn_matrix = torch.zeros(out_w, kernel_num)\n",
    "\n",
    "input_tensor_uq = input_tensor.unsqueeze(dim = 0).unsqueeze(dim = 1)\n",
    "patches = F.unfold(input_tensor_uq, kernel_num, stride=stride)\n",
    "# Reshape to get the patches in the desired format\n",
    "patches = patches.transpose(1, 2).reshape(-1, 1, kernel_num, kernel_num)\n",
    "x_offset = patches.repeat(1,out_w,1,1)\n",
    "y_offset = torch.zeros(x_offset.shape)\n",
    "\n",
    "changed_x_offset = x_offset + M\n",
    "\n",
    "x_offset[changed_x_offset < 0] = changed_x_offset[changed_x_offset < 0] + 128 \n",
    "x_offset[changed_x_offset > 128] = changed_x_offset[changed_x_offset > 128] - 256 \n",
    "conv_offset = torch.stack([y_offset, x_offset])\n",
    "print(conv_offset.shape)\n",
    "\n",
    "#print(y_offset)\n",
    "#[2, 64, 128, 7, 7] -> [1,98,64,128]\n",
    "conv_offset = torch.stack([x_offset, y_offset]).permute(3,4,0,2,1).contiguous().view(1,2*kernel_num*kernel_num,out_w,out_h) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "stride = [2, 2]\n",
    "size = [256, 64]\n",
    "\n",
    "\n",
    "h, w = size\n",
    "out_h, out_w = h // stride[0], w // stride[1]\n",
    "out_num = out_h * out_w\n",
    "\n",
    "kh, kw = 7,7\n",
    "kernel_num = kh * kw\n",
    "grid_x = torch.arange(-((kw - 1) // 2), kw // 2 + 1)\n",
    "grid_y = torch.arange(-((kh - 1) // 2), kh // 2 + 1)\n",
    "grid_x = grid_x.view(1, kw).repeat(kh, 1)\n",
    "grid_y = grid_y.view(kh, 1).repeat(1, kw)\n",
    "conv_offset = torch.stack([grid_y, grid_x]).permute(1, 2, 0).contiguous().view(-1)\n",
    "conv_offset = conv_offset.view(1, kernel_num, 2).repeat(out_num, 1, 1).type(torch.atan2(torch.tensor(1), torch.tensor(1)).type())\n",
    "conv_offset_orig = torch.stack([grid_y, grid_x]).permute(1, 2, 0).contiguous().view(-1)\n",
    "conv_offset_orig = conv_offset_orig.view(1, kernel_num, 2).repeat(out_num, 1, 1).type(torch.atan2(torch.tensor(1), torch.tensor(1)).type())\n",
    "\n",
    "# compute the offset of PolarConv\n",
    "h_index = (torch.arange(out_h) + 1) * stride[0]\n",
    "h_index = h_index - h_index[0]\n",
    "w_index = (torch.arange(out_w) + 1) * stride[1]\n",
    "w_index = w_index - w_index[0] + 1\n",
    "\n",
    "index_repeated = w_index.unsqueeze(0).repeat(out_h,1).view(-1, 1)\n",
    "conv_index = index_repeated + conv_offset[:,:,1]\n",
    "conv_index[conv_index<=0] = (w//2) / max(kw//2, 1)\n",
    "offset = w//2 / conv_index # / (max(kw//2, 1))\n",
    "conv_offset[:,:,0] *= offset\n",
    "\n",
    "# make out index to inner\n",
    "h_index_temp = h_index[:, None, None, None, None].repeat(1, out_w, kh, kw, 2).view(*conv_offset.shape)\n",
    "\n",
    "conv_offset_temp2 = conv_offset + h_index_temp\n",
    "conv_offset -= conv_offset_orig\n",
    "conv_offset[conv_offset_temp2 < 0] += h\n",
    "conv_offset[conv_offset_temp2 > h-1] -= h\n",
    "\n",
    "# if stride is bigger than 1, the center of kernel is not on the integer index.s\n",
    "# shift_h = (h - 7) % stride[0]\n",
    "# shift_w = (w - 7) % stride[1]\n",
    "# conv_offset[:, :, 0] += shift_w / 2.0\n",
    "# conv_offset[:, :, 1] += shift_h / 2.0\n",
    "\n",
    "conv_offset = conv_offset.contiguous().view(1, out_h, out_w, 2 * kernel_num).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 98, 128, 32])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_offset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -6.0000,   -6.0000,   -6.0000,  -93.0000,  -45.0000,  -29.0000,\n",
       "          -21.0000],\n",
       "        [  -4.0000,   -4.0000,   -4.0000,  -62.0000,  -30.0000,  -19.3333,\n",
       "          -14.0000],\n",
       "        [  -2.0000,   -2.0000,   -2.0000,  -31.0000,  -15.0000,   -9.6667,\n",
       "           -7.0000],\n",
       "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
       "            0.0000],\n",
       "        [   2.0000,    2.0000,    2.0000,   31.0000,   15.0000,    9.6667,\n",
       "            7.0000],\n",
       "        [   4.0000,    4.0000,    4.0000, -194.0000,   30.0000,   19.3333,\n",
       "           14.0000],\n",
       "        [   6.0000,    6.0000,    6.0000, -163.0000,   45.0000,   29.0000,\n",
       "           21.0000]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_offset[0,:,100,0].reshape(7,7,2)[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/media/ddd/git/spa_AeDet/outputs/overfit/metrics_summary.json') as file:\n",
    "    # Load the JSON data\n",
    "    data = json.load(file)\n",
    "print(\"mAP:\", round(data['mean_ap'], 4))\n",
    "print(\"mATE:\", round(data['tp_errors']['trans_err'], 4))\n",
    "print(\"mASE:\", round(data['tp_errors']['scale_err'], 4))\n",
    "print(\"mAOE:\", round(data['tp_errors']['orient_err'], 4))\n",
    "print(\"mAVE:\", round(data['tp_errors']['vel_err'], 4))\n",
    "print(\"mAOE:\", round(data['tp_errors']['attr_err'], 4))\n",
    "print(\"NDS:\", round(data['nd_score'], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP: 0.4808\n",
      "mATE: 0.6275\n",
      "mASE: 0.4517\n",
      "mAOE: 0.3778\n",
      "mAVE: 0.4269\n",
      "mAOE: 0.3752\n",
      "NDS: 0.5145\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[721.]])\n",
      "tensor([[[[0., 0.]]]])\n",
      "tensor([[[[771.5000]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input feature map\n",
    "# This could represent a 1x1 image with 1 channel (for simplicity)\n",
    "input = torch.ones(1, 1, 16, 44)\n",
    "\n",
    "# Flow field grid\n",
    "# In this case, it's a 1x1 grid with an offset of 0.2 in the x and y directions\n",
    "# The values should be in the range of [-1, 1], where -1 represents the left/top boundary, 1 represents the right/bottom boundary, and 0 is the center.\n",
    "grid = torch.Tensor([[[[0,0]]]])\n",
    "\n",
    "for i in range(16):\n",
    "    for j in range(44):\n",
    "        input[:,:,i,j] = 100*i + j\n",
    "# Resample the input based on the grid\n",
    "output = F.grid_sample(input, grid, mode='bilinear', padding_mode='zeros')\n",
    "\n",
    "print(input[:,:,7,21])\n",
    "print(grid)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1243])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create a Conv2d layer\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "\n",
    "# Print the initial bias values\n",
    "print(conv.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image format: JPEG\n",
      "Image size: (1280, 800)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open the image file\n",
    "image = Image.open('/media/ddd/shift/discrete/images/val/front/img/6c37-2ac8/00000030_img_front.jpg')\n",
    "\n",
    "# Display some information about the image\n",
    "print('Image format:', image.format)\n",
    "print('Image size:', image.size)\n",
    "\n",
    "# You can perform various operations on the image here\n",
    "# For example, you can save it in a different format or display it\n",
    "\n",
    "# Close the image file\n",
    "image.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    " \n",
    "smtp = smtplib.SMTP('spa.hanyang.ac.kr', 587)\n",
    "smtp.ehlo()      # say Hello\n",
    "smtp.starttls()  # TLS 사용시 필요\n",
    "smtp.login('junghokim@spa.hanyang.ac.kr', 'kjh9653102')\n",
    " \n",
    "msg = MIMEText('본문 테스트 메시지')\n",
    "msg['Subject'] = '테스트'\n",
    "msg['To'] = 'junghokim@spa.hanyang.ac.kr'\n",
    "smtp.sendmail('junghokim@spa.hanyang.ac.kr', 'junghokim@spa.hanyang.ac.kr', msg.as_string())\n",
    " \n",
    "smtp.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
